---
title: "Linear regression using glm, manual ML optimization, and `optim`"
author: "Petr Keil"
date: "March 2017"
output:
  html_document:
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
  pdf_document: default
---

![](https://raw.githubusercontent.com/petrkeil/ML_and_Bayes_2017_iDiv/master/FIgures/normal_linear_regression.png)

The figure is from Kruschke (2014) Doing Bayesian Data Analysis, 2nd edition, Academic Press / Elsevier.


```{r}
xy <- read.csv("https://raw.githubusercontent.com/petrkeil/ML_and_Bayes_2017_iDiv/master/Manual_ML_regression/xy_data.csv")

xy
```

# The data

We will use data from **Michael Crawley's R Book**, Chapter 10 (Linear Regression). The data show the growth of catepillars fed on experimental diets differing in their tannin contnent.

![danaus caterpillar figure](figure/danaus.png)
![danaus caterpillar figure](figure/Tannic_acid.png)

To load the data to R directly from the web:

```{r}
  catepil <- read.table("http://www.petrkeil.com/wp-content/uploads/2016/01/regression.txt", sep="\t", header=TRUE)
  catepil
```

The data look like this:

```{r, fig.width=4, fig.height=4}
  plot(growth~tannin, data=catepil)
```

# The model

The classical notation:
$$ growth_i = a + b \times tannin_i + \epsilon_i  $$
$$ \epsilon_i \sim Normal(0, \sigma)$$


An alternative version:
$$ \mu_i = a + b \times tannin_i $$
$$ growth_i \sim Normal(\mu_i, \sigma) $$

**Note:** The notations are mathematically equivalent, 
but the Bayesian notation shows, in my opinion, more directly 
how we think about the stochastic part of the model.

# Fitting ordinary least squares (OLS) regression

```{r, fig.width=4, fig.height=4}
  model.lm <- lm(growth~tannin, data=catepil)
  plot(growth~tannin, data=catepil)
  abline(model.lm)
  summary(model.lm)
```



# Fitting linear regression manually

```{r, eval=FALSE}

library(manipulate)

myplot <- function(xy, a, b, sigma)
{
  x <-xy$x
  mean.y <- a + b*x
  low.y  <- qnorm(0.025, mean.y, sigma)
  up.y <- qnorm(0.975, mean.y, sigma)
  
  neg.LL <- - sum(dnorm(xy$y, mean=mean.y, sd=sigma, log=TRUE))
  neg.LL <- round(neg.LL, 3)
  
  plot(xy, main=paste("Neg. LL =", neg.LL))
  
  lines(x, mean.y, col="red")
  lines(x, low.y, lty=2, col="red")
  lines(x, up.y, lty=2, col="red")
}
```

```{r, eval=FALSE}
manipulate(
myplot(xy, a, b, sigma),
a = slider(min=-5, max=5, step=0.01, initial=3),
b = slider(min=-5, max=5, step=0.01, initial=4),
sigma = slider(min=0, max=3, step=0.01, initial=0.1)
)
```

# Linear regression fitted with ML and `optim`

```{r}
neg.LL.function.for.optim <- function(par, dat)
{
  x <- dat$x
  y <- dat$y
  a <- par[1]
  b <- par[2]
  sd <- par[3]
  LL <- dnorm(y, mean=a + b*x, sd=sd, log=TRUE)
  neg.LL <- - sum(LL)
  return(neg.LL)
}


optim(par=c(a=0, b=0, sd=1), 
      fn=neg.LL.function.for.optim, 
      dat=xy)

```
















